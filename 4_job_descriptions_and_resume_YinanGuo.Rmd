---
title: "4_job_descriptions_and_resume.Rmd "
author  : "Yinan Guo"
date    : "2021-07-31" 
output: 
  html_document:
    toc: true
    toc_depth: 3
    theme: paper
    highlight: tango
---

## Libraries 
Load the following libraries, you may need to install a few!

library(tidyverse)
library(readxl)
library(tidytext)
library(wordcloud2)
library(janitor)
tidytext, tm, tidyr, wordcloud, wordcloud2, readxl and pdftools
There is a bug in wordcloud2 which prevents it from knitting more than 1 wordcloud, so we want to install wordcloud2 using devtools from github using  devtools::install_github("gaospecial/wordcloud2"). Just do this once. if you don't have devtools installed, install it first then install wordcloud2 like this.

devtools::install_github("gaospecial/wordcloud2")

```{r}
library(tidyverse)
library(readxl)
library(tidytext)
library(wordcloud2)
library(janitor)
library(tm)
library(tidyr)
library(pdftools)
```



# Setup 

-------- 

Part 1. Analyze Jobs 

## 1. Import 

1. Create jobs dataframe by using read_excel() to read in your MSBA30-50Jobs.xlsx (after you've loaded it with jobs), you can get started by using the MSBA20JobDescription.xlsx file if you want. 
2. I recommend piping it to clean_names() to deal with column names 
3. Print the first 5 jobs to make sure your import works. 

```{r}
msbajd <- read_excel("data/MSBA20JobDescriptions.xlsx") %>%
  clean_names()
head(msbajd,5)
```


# Task 1 – Job Description Term & Bigram Frequency Analysis 

--------

## Task 1.Create a table of term frequencies 

The following steps will help you make your  term_frequency table need to make your first wordcloud. 

0. make a vector called excludes <- this is just an exmaple c("key", "clients", "chicago")
1. pipe jobs into unnest_tokens(word, job_description)
2. pipe into anti_join(stop_words, by = c("word" = "word")) this will remove common words
3. pipe into filter(!word %in% excludes ) this will remove excluded words  
  - remove "key", "clients", "chicago" and any other words you think make sense
  
4. pipe into filter(!str_detect(word,"^\\d")) this will remove digits 
5. group_by(word) and summarize or use count()
6. arrange(desc(n)) 
7. create a term_frequency table 
8. finally, print out the the top 20 terms using top_n() or slice_max()


```{r}
excludes <-c("key", "clients", "chicago","work","job","working","ability","and","to","of","with","in","the","at","a","is","are","be","have","on","will","for","as","then","this","that","you","if","do","we","or","our","your")

msbajd_freq <- msbajd %>%
  unnest_tokens(word, job_description) %>%
  anti_join(get_stopwords(language="en")) %>%
  filter(!word %in% excludes) %>%
  filter(!str_detect(word,"^\\d")) %>% 
  group_by(word) %>%
  count(word, sort=TRUE) %>%
  ungroup() %>%
  arrange(desc(n))

msbajd_freq %>%
  slice_max(n,n=20)
```

### Task 1a. 

Create a wordcloud with all of the terms, using wordcloud2()

```{r}
msbajd_freq %>%
  wordcloud2()
```

### Task 1b. 

Create a wordcloud for the terms that start with the letter "a" 

```{r}
msbajd_freq %>%
  filter(startsWith(word,'a')) %>%
  wordcloud2()
```

### Task 1c. 

Create a word cloud of companies 

```{r}
msbafirm_freq <- msbajd %>%
  unnest_tokens(word, firm) %>%
  anti_join(get_stopwords(language="en")) %>%
  filter(!str_detect(word,"^\\d")) %>% 
  group_by(word) %>%
  count(word, sort=TRUE) %>%
  ungroup() %>%
  arrange(desc(n))

msbafirm_freq %>%
  wordcloud2()
```


### Task 1d. 

Create a word cloud of job titles. 

```{r}
msbajobs_freq <- msbajd %>%
  unnest_tokens(word, title) %>%
  anti_join(get_stopwords(language="en")) %>%
  filter(!str_detect(word,"^\\d")) %>% 
  group_by(word) %>%
  count(word, sort=TRUE) %>%
  ungroup() %>%
  arrange(desc(n))

msbajobs_freq %>%
  wordcloud2()
```



# Task 2, Words after DATA 

Clearly "DATA" an important word so what words come after data? 

------

## Task 2.1 - Words AFTER "data" ... 

1. pipe jobs 
2. into: unnest_tokens(bigram, job_description, token = "ngrams", n = 2, n_min = 2), what does bigram do?
3. separate bigram into two words: separate(bigram, c("word1", "word2"), sep = " ")
4. filter for "data" filter(word1 == "data") 
5. remove junk words on word2, filter(!word2 %in% excludes ) this will remove excluded word
6. unite word 1 and 2 together into a bigram
7. group_by(bigram)
8. summarize(n=n())
9. arrange(desc(n))
10. save data_term_frequency 

11. print top 10 data + terms 

```{r}
msbajd %>%
  unnest_tokens(bigram, job_description, token = "ngrams", n = 2, n_min = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(word1 == "data") %>%
  filter(!word2 %in% excludes ) %>%
  mutate(bigram = str_c( word1, word2, sep = " ")) %>%
  group_by(bigram) %>%
  summarize(n=n()) %>%
  arrange(desc(n)) ->data_term_frequency

data_term_frequency %>%
  slice_max(n,n=10)

```

## Task 2.2 - Create a word cloud of data + term combinations

```{r}
data_term_frequency %>%
  wordcloud2()
```

## Task 2.3 - Create a bar chart of the top 15, data + term combinations

```{r}
data_term_frequency %>%
  arrange(desc(n))%>%
  top_n(15) %>%
  ggplot(aes(reorder(bigram, n),n)) +
  geom_bar(stat='identity') +
  coord_flip() +
  labs(title="top 15 data + term combinations ",
        x ="data + term combinations", 
        y = "times") 

```

# Task 3, Technology Term Analysis  

Analyze Technology Terms 
--------

## Task 3 Instructions. 

Here is a list of important technology terms - you of course are free to add more. 
1. load them up. 
2. add any additional technology term or bigram that you think are useful. 

```{r}
technology_words <- c(
    "analytics", 
    "data",
    "analyze",
    "r", 
    "python", 
    "sql", 
    "excel", 
    "cloud",
    "aws",
    "azure",
    "ec2",
    "sas",
    "spss",
    "saas",
    "spark",
    "tensorflow",
    "sagemaker",
    "tableau",
    "hadoop",
    "pyspark",
    "h2o.ai",
    "spark", 
    "ai",
    "shiny",
    "dash",
    "pca",
    "k-means",
    "emr",
    "mapreduce",
    "nosql",
    "hive"
    )


technology_bigram <- c(
  "amazon web",
  "big data",
  "business analytics",
  "google cloud",
  "microsoft azure",
  "machine learning",
  "data science",
  "deep learning",
  "neural network",
  "neural networks",
  "neural nets",
  "random forests",
  "random forest",
  "elastic search",
  "map reduce",
  "artificial intelligence"
)


```


## Create Word & Bi-Gram Frequencies - Jobs 

1.	Filter the term_frequency table based on the technology_terms provided, tech_term_freq
2.	Filter the bigram_frequency table based on technology_bigrams provided, tech_bigram_freq
3.	Smash the results together into technology_term_frequency using using bind_rows()

```{r}
tech_term_freq <-
  msbajd_freq %>%
  filter(word %in% technology_words)
tech_term_freq

msbabigram_freq <-
  msbajd %>%
  unnest_tokens(bigram, job_description, token = "ngrams", n = 2, n_min = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% excludes) %>%
  filter(!word2 %in% excludes ) %>%
  mutate(word = str_c( word1, word2, sep = " ")) %>%
  group_by(word) %>%
  summarize(n=n()) %>%
  arrange(desc(n)) 

tech_bigram_freq <-
  msbabigram_freq %>%
  filter(word %in% technology_bigram)
tech_bigram_freq

technology_term_frequency <-
  bind_rows(list(tech_term_freq,tech_bigram_freq))
technology_term_frequency
  
```

### Task 3a. Make a Bar Chart of Technolgy Terms 

```{r}
technology_term_frequency %>%
  ggplot(aes(reorder(word, n),n)) +
  geom_bar(stat='identity') +
  coord_flip() +
  labs(title="Technolgy Terms  ",
        x ="Technolgy Terms ", 
        y = "times") 
```

### Task 3b. Make a Wordcloud of Techology Terms 

```{r}
technology_term_frequency %>%
  wordcloud2()
```


# Task 4. Your Resume 

---------

## Task 4 - getting started 

1. save/print your resume as PDF, it needs to be PDF format for this to work.  
2. install pdftools and load the library(pdftools)
3. use pdf_text to read your resume into a table. for example here's how i read my resume:  

resume <- pdf_text("/Users/mikames/Downloads/Michael Ames Resume_CurrentJan2019.pdf")
  
> note: this makes a character vector of your resume, we need to tidy it up and get it into a table. 
  
```{r}
resume <- pdf_text("data/YinanGuo-Resume.pdf")
```
  
## Task 4.2 - Parse your resume, filter out the common words, digits, and any excludes 
Your resume is now a character vector, you'll need to perform the following 
1. as_tibble(resume) will convert it to a data frame 
2. parse text into words, using unnest_tokens(word, value)
3. remove words you want to exclude 
4. remove numbers
5. group by words and count the words up. to make a resume_word_freq table.  

```{r}
resume %>%
  as_tibble() %>%
  unnest_tokens(word, value) %>%
  anti_join(get_stopwords(language="en")) %>%
  filter(!word %in% excludes) %>%
  filter(!str_detect(word,"^\\d")) %>% 
  filter(!word %in% c("ca","b","c","guo","guoy221","san","diego","beijing","china","london","mar","may","aug","seu","jan","sep","apr","jul","jun","oct")) %>%
  group_by(word) %>%
  count(word, sort=TRUE) %>%
  ungroup() %>%
  arrange(desc(n)) ->resume_word_freq
resume_word_freq

#filter for technology_word
resume_tech_word_freq <-
  resume_word_freq %>%
  filter(word %in% technology_words ) 
resume_tech_word_freq
```

## Task 4.3 - Create a word cloud of the remaining words in your resume

```{r}
resume_word_freq %>%
  wordcloud2()
```

## Task 4.4 filter for technology_words and technology_bigrams 

to do this you'll need to Bigram Freq. your resume, hen filter for technology_bigrams 

###  Task 4.4 Steps: Bigram Freq. your resume, filter for technology_bigrams  

1. as_tibble(resume) pipe into 
2. unnest_tokens(word, value, token = "ngrams", n = 2, n_min = 2) to make bigrams 
3. filter(word %in% technology_bigram ) for specific bigrams 
4. group_by(word) 
5. summarize(n=n()) 
6.  arrange(desc(n)) 
7.  make a resume_bigram_freq


```{r}
resume %>%
  as_tibble() %>%
  unnest_tokens(word, value, token = "ngrams", n = 2, n_min = 2) %>%
  filter(word %in% technology_bigram ) %>%
  group_by(word) %>%
  summarize(n=n()) %>%
  arrange(desc(n)) ->resume_bigram_freq

resume_bigram_freq
```

### 4.4 finally SMASH resume_word_freq & resume_bigram_freq together 

1. use bind_rows() to do this. 


```{r}
resume_term_frequency <-
  bind_rows(list(resume_tech_word_freq,resume_bigram_freq))
resume_term_frequency 
```

### TASK 4.4a - make a bar chart of combined technolgy term frequencies

```{r}
resume_term_frequency %>%
  ggplot(aes(reorder(word, n),n)) +
  geom_bar(stat='identity') +
  coord_flip() +
  labs(title="combined technolgy term frequencies  ",
        x ="Resume Terms ", 
        y = "times")
```


### TASK 4.4b - make a wordcloud of combined technolgy term frequencies

```{r}
resume_term_frequency %>%
  wordcloud2()
```

## Task 5 – Compare resume jobs. 

5.1	What terms do your resume and jobs data have in common? That is, compare your resume's terms to the terms found in the job descriptions. 
```{r}
myjd <- read_excel("data/My20JobDescriptions_YinanGuo.xlsx") %>%
  clean_names()

myjd_freq <- myjd %>%
  unnest_tokens(word, job_description) %>%
  anti_join(get_stopwords(language="en")) %>%
  filter(!word %in% excludes) %>%
  filter(!str_detect(word,"^\\d")) %>% 
  group_by(word) %>%
  count(word, sort=TRUE) %>%
  ungroup() %>%
  arrange(desc(n))
myjd_freq

resume_word_freq %>%
  filter(word %in% myjd_freq$word) %>%
  select(word)
```

5.2	Based on the job's terms, what terms are missing from your resume? Make a table of terms missing from your resume but found in your job descriptions. 
```{r}
myjd_freq %>%
  filter(!word %in% resume_word_freq$word) %>%
  select(word)
```

5.3	What tech-skills does your resume and jobs have in common?
```{r}
#resume_tech_word_freq is tech words within resume
myjd_freq %>%
  filter(word %in% resume_tech_word_freq$word) %>%
  select(word)
```

5.4	Based on the job's tech-skills what skills are missing from your resume? 

```{r}
myjd_freq %>%
  filter(word %in% technology_words) %>%
  filter(!word %in% resume_tech_word_freq$word) %>%
  select(word)
```

#Questions in instruction Word file but not in this Rmd:
•	What are the top 10 key words that your jobs found vs. the MSBA20 job description file? 
•	What are the top 5 technology terms that you found vs the MSBA20 job description file? 

```{r}
msbajd_freq %>% slice_max(n,n=10)
myjd_freq %>% slice_max(n,n=10)

tech_term_freq %>% slice_max(n,n=5)
myjd_tech_term_freq <-
  myjd_freq %>%
  filter(word %in% technology_words) 
myjd_tech_term_freq %>% slice_max(n,n=5)
```

